# Chapter 3: State Estimation

## Overview

This module implements the state estimation algorithms described in **Chapter 3** of *Principles of Indoor Positioning and Indoor Navigation*. It provides the mathematical foundations for estimating position, velocity, and other states from noisy measurements using various filtering and optimization techniques.

## Equation Mapping: Code ‚Üî Book

The following table maps the implemented functions to their corresponding equations in Chapter 3 of the book:

### Least Squares Methods

| Function | Location | Equation | Status | Description |
|----------|----------|----------|--------|-------------|
| `linear_least_squares()` | `core/estimators/least_squares.py` | **Eq. (3.1)** | ‚úì | Standard LS: xÃÇ = (A'A)‚Åª¬πA'b |
| `weighted_least_squares()` | `core/estimators/least_squares.py` | **Eq. (3.2)** | ‚úì | WLS with measurement covariance: xÃÇ = (A'WA)‚Åª¬πA'Wb |
| `iterative_least_squares()` | `core/estimators/least_squares.py` | **Eq. (3.3)** | ‚úì | Gauss-Newton for nonlinear problems |
| `robust_least_squares()` | `core/estimators/least_squares.py` | **Eq. (3.4)** | ‚úì | IRLS with Huber/Cauchy/Tukey loss functions |

### Kalman Filtering

| Function | Location | Equation | Status | Description |
|----------|----------|----------|--------|-------------|
| `KalmanFilter.predict()` | `core/estimators/kalman_filter.py` | **Eq. (3.5)** | üöß | Linear KF prediction: xÃÑ‚Çñ = Fx‚Çñ‚Çã‚ÇÅ + Bu‚Çñ |
| `KalmanFilter.update()` | `core/estimators/kalman_filter.py` | **Eq. (3.6)** | üöß | Linear KF update: x‚Çñ = xÃÑ‚Çñ + K(z - HxÃÑ‚Çñ) |
| `KalmanFilter._compute_kalman_gain()` | `core/estimators/kalman_filter.py` | **Eq. (3.7)** | üöß | Kalman gain: K = PÃÑH'(HPÃÑH' + R)‚Åª¬π |

### Extended Kalman Filter (EKF)

| Function | Location | Equation | Status | Description |
|----------|----------|----------|--------|-------------|
| `ExtendedKalmanFilter.predict()` | `core/estimators/extended_kalman_filter.py` | **Eq. (3.8)** | üöß | Nonlinear prediction with Jacobian F‚Çñ |
| `ExtendedKalmanFilter.update()` | `core/estimators/extended_kalman_filter.py` | **Eq. (3.9)** | üöß | Nonlinear update with measurement Jacobian H‚Çñ |

### Unscented Kalman Filter (UKF)

| Function | Location | Equation | Status | Description |
|----------|----------|----------|--------|-------------|
| `UnscentedKalmanFilter._compute_sigma_points()` | `core/estimators/unscented_kalman_filter.py` | **Eq. (3.10)** | ‚è≥ | Sigma point generation |
| `UnscentedKalmanFilter.predict()` | `core/estimators/unscented_kalman_filter.py` | **Eq. (3.11)** | ‚è≥ | UT-based prediction |
| `UnscentedKalmanFilter.update()` | `core/estimators/unscented_kalman_filter.py` | **Eq. (3.12)** | ‚è≥ | UT-based measurement update |

### Particle Filter (PF)

| Function | Location | Equation | Status | Description |
|----------|----------|----------|--------|-------------|
| `ParticleFilter.predict()` | `core/estimators/particle_filter.py` | **Eq. (3.13)** | ‚è≥ | Particle propagation: x·µ¢‚Çñ ~ p(x‚Çñ\|x·µ¢‚Çñ‚Çã‚ÇÅ) |
| `ParticleFilter.update()` | `core/estimators/particle_filter.py` | **Eq. (3.14)** | ‚è≥ | Importance weighting: w·µ¢‚Çñ ‚àù p(z‚Çñ\|x·µ¢‚Çñ) |
| `ParticleFilter.resample()` | `core/estimators/particle_filter.py` | **Eq. (3.15)** | ‚è≥ | Systematic resampling |

### Performance Metrics

| Function | Location | Equation | Status | Description |
|----------|----------|----------|--------|-------------|
| `compute_innovation()` | `core/eval/metrics.py` | **Eq. (3.16)** | ‚è≥ | Innovation: ŒΩ = z - ·∫ë |
| `compute_nees()` | `core/eval/metrics.py` | **Eq. (3.17)** | ‚è≥ | Normalized Estimation Error Squared |
| `compute_nis()` | `core/eval/metrics.py` | **Eq. (3.18)** | ‚è≥ | Normalized Innovation Squared |

**Legend:**
- ‚úì = Fully implemented and tested
- üöß = Implementation in progress
- ‚è≥ = Planned (not yet implemented)
- ‚úó = Not implemented (out of scope)

## Implementation Notes

### ‚úì Fully Implemented

#### 1. **Linear Least Squares (LS)**
   - Solves overdetermined systems: Ax = b where m ‚â• n
   - Uses normal equations: (A'A)x = A'b
   - Computes covariance: P = œÉ¬≤(A'A)‚Åª¬π
   - Handles exact fit cases (m = n) and overdetermined (m > n)
   - **All 6 test cases pass**

#### 2. **Weighted Least Squares (WLS)**
   - Incorporates measurement uncertainties via weight matrix W
   - Typically W = R‚Åª¬π (inverse of measurement covariance)
   - Validates W is symmetric positive definite
   - Reduces to LS when W = I
   - **All 5 test cases pass**

#### 3. **Iterative Least Squares (Gauss-Newton)**
   - Handles nonlinear measurement models: f(x) ‚âà b
   - Linearizes at each iteration using Jacobian J = ‚àÇf/‚àÇx
   - Converges quadratically near solution
   - Configurable max iterations and tolerance
   - **All 4 test cases pass** including 2D range positioning

#### 4. **Robust Least Squares (IRLS)**
   - Three loss functions implemented:
     - **Huber**: Quadratic for small residuals, linear for large
     - **Cauchy**: Heavy-tailed, more aggressive outlier rejection
     - **Tukey**: Sets outliers to zero weight beyond threshold
   - Uses Median Absolute Deviation (MAD) for robust scale estimation
   - Iteratively reweights and converges to stable solution
   - **All 7 test cases pass** including multi-outlier scenarios

### üöß In Progress

#### 5. **Kalman Filter (Linear KF)**
   - Optimal estimator for linear Gaussian systems
   - Prediction step (time update)
   - Update step (measurement correction)
   - Covariance propagation

### Implementation Choices

#### 1. **Numerical Stability**
   - Rank checking before solving normal equations
   - Pseudo-inverse fallback for near-singular matrices
   - MAD-based robust scale estimation (more robust than std dev)
   - Clipping of weights to valid range [0, 1]

#### 2. **Convergence Criteria**
   - Iterative LS: ||Œîx|| < tolerance
   - Robust LS: ||x_new - x_old|| < tolerance
   - Default tolerance: 1e-6 for positions (millimeter accuracy)

#### 3. **Error Handling**
   - Validates matrix dimensions before computation
   - Checks rank deficiency
   - Detects non-positive-definite covariance matrices
   - Provides informative error messages

#### 4. **Testing Philosophy**
   - Each function has 4-7 comprehensive test cases
   - Tests cover: exact fit, noisy data, outliers, edge cases
   - Numerical accuracy verified to < 1e-6 for most cases
   - Round-trip tests for consistency

## File Structure

```
ch3_estimators/
‚îú‚îÄ‚îÄ README.md                              # This file
‚îú‚îÄ‚îÄ example_least_squares.py               # LS/WLS/ILS/Robust LS demonstrations
‚îú‚îÄ‚îÄ example_kalman_1d.py                   # 1D constant velocity tracking
‚îú‚îÄ‚îÄ example_ekf_range_bearing.py           # 2D positioning with EKF
‚îî‚îÄ‚îÄ example_comparison.py                  # Compare estimators

core/estimators/
‚îú‚îÄ‚îÄ __init__.py                            # Package exports
‚îú‚îÄ‚îÄ base.py                                # Abstract base classes
‚îú‚îÄ‚îÄ least_squares.py                       # LS/WLS/ILS/Robust LS [DONE]
‚îú‚îÄ‚îÄ kalman_filter.py                       # Linear KF [TODO]
‚îú‚îÄ‚îÄ extended_kalman_filter.py              # EKF [TODO]
‚îú‚îÄ‚îÄ unscented_kalman_filter.py             # UKF [TODO]
‚îî‚îÄ‚îÄ particle_filter.py                     # PF [TODO]

tests/core/estimators/
‚îú‚îÄ‚îÄ test_least_squares.py                  # 22 test cases [DONE]
‚îú‚îÄ‚îÄ test_kalman_filter.py                  # [TODO]
‚îú‚îÄ‚îÄ test_extended_kalman_filter.py         # [TODO]
‚îî‚îÄ‚îÄ test_particle_filter.py                # [TODO]

data/sim/ch3/
‚îú‚îÄ‚îÄ range_measurements.npz                 # Simulated TOA data
‚îú‚îÄ‚îÄ range_bearing.npz                      # Range + bearing data
‚îú‚îÄ‚îÄ tracking_1d.npz                        # 1D motion data
‚îî‚îÄ‚îÄ landmark_map.npz                       # 2D SLAM scenario
```

## Usage Examples

### Example 1: Linear Least Squares (Position from Ranges)

```python
import numpy as np
from core.estimators import linear_least_squares

# 2D positioning from 4 range measurements
# Anchors at corners of 10m √ó 10m square
anchors = np.array([[0, 0], [10, 0], [0, 10], [10, 10]])
true_position = np.array([3.0, 4.0])

# Measured ranges (with noise)
ranges = np.linalg.norm(anchors - true_position, axis=1) + 0.1 * np.random.randn(4)

# Linearized problem: solve for position offset from initial guess
x0 = np.array([5.0, 5.0])  # Initial guess at center

# Design matrix (Jacobian of range function)
def compute_design_matrix(x_ref, anchors):
    diff = x_ref - anchors
    ranges_ref = np.linalg.norm(diff, axis=1, keepdims=True)
    return diff / ranges_ref

A = compute_design_matrix(x0, anchors)
b = ranges - np.linalg.norm(anchors - x0, axis=1)

# Solve for offset
dx, P = linear_least_squares(A, b)
position_estimate = x0 + dx

print(f"True position: {true_position}")
print(f"Estimated: {position_estimate}")
print(f"Error: {np.linalg.norm(position_estimate - true_position):.3f} m")
```

**Implements:** Eq. (3.1)

### Example 2: Weighted Least Squares (Different Measurement Accuracies)

```python
from core.estimators import weighted_least_squares

# Same setup, but with different measurement uncertainties
# Anchor 0 is very accurate (œÉ=0.1m), others are noisy (œÉ=1.0m)
measurement_std = np.array([0.1, 1.0, 1.0, 1.0])

# Weight matrix is inverse of measurement covariance
W = np.diag(1.0 / measurement_std**2)

# WLS solution weights accurate measurements more
dx_wls, P_wls = weighted_least_squares(A, b, W)
position_wls = x0 + dx_wls

print(f"Weighted LS estimate: {position_wls}")
print(f"Covariance trace: {np.trace(P_wls):.6f}")
```

**Implements:** Eq. (3.2)

### Example 3: Iterative Least Squares (Nonlinear Range Positioning)

```python
from core.estimators import iterative_least_squares

# Define nonlinear measurement model
def range_model(x):
    """Compute predicted ranges from position x to all anchors."""
    return np.linalg.norm(anchors - x, axis=1)

def range_jacobian(x):
    """Jacobian of range function: ‚àÇr/‚àÇx."""
    diff = x - anchors
    ranges = np.linalg.norm(diff, axis=1, keepdims=True)
    return diff / np.maximum(ranges, 1e-10)  # Avoid division by zero

# Initial guess
x_init = np.array([5.0, 5.0])

# Iterative solution
x_hat, P, iterations = iterative_least_squares(
    range_model, range_jacobian, ranges, x_init, max_iter=10, tol=1e-6
)

print(f"Converged in {iterations} iterations")
print(f"Position: {x_hat}")
print(f"Error: {np.linalg.norm(x_hat - true_position):.3f} m")
```

**Implements:** Eq. (3.3) - Gauss-Newton algorithm

### Example 4: Robust Least Squares (With Outliers)

```python
from core.estimators import robust_least_squares

# Add a severe outlier to one measurement
ranges_with_outlier = ranges.copy()
ranges_with_outlier[2] += 5.0  # 5m outlier

# Standard LS (corrupted by outlier)
dx_ls, _ = linear_least_squares(A, ranges_with_outlier - np.linalg.norm(anchors - x0, axis=1))
position_ls = x0 + dx_ls

# Robust LS (downweights outlier)
dx_robust, P_robust, weights = robust_least_squares(
    A, 
    ranges_with_outlier - np.linalg.norm(anchors - x0, axis=1),
    method="huber",
    threshold=2.0
)
position_robust = x0 + dx_robust

print(f"Standard LS error: {np.linalg.norm(position_ls - true_position):.3f} m")
print(f"Robust LS error: {np.linalg.norm(position_robust - true_position):.3f} m")
print(f"Outlier weight: {weights[2]:.3f}")  # Should be << 1.0
```

**Implements:** Eq. (3.4) - IRLS with Huber loss

## Running the Examples

### Run All Tests

```bash
# Run all Chapter 3 tests
pytest tests/core/estimators/ -v

# Run specific module
pytest tests/core/estimators/test_least_squares.py -v

# Run with coverage
pytest tests/core/estimators/ --cov=core.estimators --cov-report=html
```

**Test Coverage:**
- 22 test cases for least squares methods
- All tests pass with numerical accuracy < 1e-6
- Edge cases: rank deficiency, outliers, convergence

### Demo Scripts

```bash
cd ch3_estimators
python example_least_squares.py
python example_comparison.py
```

## Verification and Validation

### Least Squares Tests

| Test Case | Expected Result | Actual Result | Status |
|-----------|----------------|---------------|--------|
| Exact fit (y = 2x + 1) | [1.0, 2.0] | [1.0, 2.0] (< 1e-10) | ‚úì |
| Overdetermined (5 equations, 2 unknowns) | Close to true | Error < 0.1 | ‚úì |
| Identity matrix | Return b exactly | |b - xÃÇ| < 1e-10 | ‚úì |
| Rank deficient | Raise ValueError | ValueError raised | ‚úì |
| WLS equal weights = LS | x_wls ‚âà x_ls | |x_wls - x_ls| < 1e-10 | ‚úì |
| WLS emphasizes accurate | Close to accurate meas. | Verified | ‚úì |
| Iterative LS on linear | Converges to LS | < 5 iterations | ‚úì |
| Range positioning 2D | Error < 1e-6 | < 10 iterations | ‚úì |
| Robust LS (1 outlier) | Downweights outlier | w_outlier < 0.5 | ‚úì |
| Huber vs standard LS | Robust < standard error | Verified | ‚úì |

## Differences from Book

### Simplifications

1. **Robust Loss Functions**
   - Book may describe M-estimators in general form
   - Implementation provides three specific functions (Huber, Cauchy, Tukey)
   - MAD used for robust scale (simpler than iterative scale estimation)

2. **Iterative LS**
   - Book may cover Levenberg-Marquardt or trust-region methods
   - Implementation uses Gauss-Newton (simpler, sufficient for most cases)
   - Fallback to pseudo-inverse for singular Jacobians

### Extensions

1. **Comprehensive Testing**
   - Each function has 4-7 test cases covering edge cases
   - Automatic validation of matrix properties (symmetry, positive definiteness)
   - Clear error messages for common mistakes

2. **Configurable Parameters**
   - Max iterations and tolerances are adjustable
   - Multiple robust loss functions available
   - Optional covariance computation (saves computation when not needed)

### Not Implemented

1. **Advanced Optimization**
   - Levenberg-Marquardt algorithm (LM) for better convergence
   - Trust-region methods
   - Line search for step size control

2. **Specialized Algorithms**
   - Sequential least squares
   - Recursive least squares (RLS)
   - Total least squares (TLS)

## References

- **Chapter 3**: State Estimation
  - Section 3.2: Least Squares Methods
  - Section 3.3: Kalman Filtering
  - Section 3.4: Nonlinear Filters (EKF, UKF)
  - Section 3.5: Particle Filters

- **Numerical Recipes**: Press et al. (2007)
- **Robust Statistics**: Huber & Ronchetti (2009)

## Future Work

1. **Implement Kalman Filter** (linear KF)
2. **Implement Extended Kalman Filter** (EKF)
3. **Implement Unscented Kalman Filter** (UKF)
4. **Implement Particle Filter** (PF)
5. **Add performance metrics** (NEES, NIS, innovation tests)
6. **Create simulation data generators**
7. **Add interactive Jupyter notebooks**

## Contributing

When adding new estimators:

1. **Add equation reference** in docstring (e.g., "Implements Eq. (3.X)")
2. **Update this README** with new mapping entry
3. **Add comprehensive unit tests** (minimum 4-5 test cases)
4. **Verify numerical accuracy** (< 1e-6 for typical cases)
5. **Update `core/estimators/__init__.py`** with exports

---

**Status**: ‚úì Least Squares methods fully implemented and tested  
**Last Updated**: December 2025  
**Maintainer**: Navigation Engineering Team

