




Indoor Simultaneous Localization and Mapping (SLAM)

Chapter Outline

7.1	Introduction to SLAM
7.1.1	Historical Development of SLAM
7.1.2	SLAM Frameworks and Evolution
7.1.3	Comparison of Popular SLAM Implementations
7.2	General Mathematical Model of SLAM
7.3	LiDAR SLAM
7.3.1	Point-cloud based LiDAR SLAM - ICP
7.3.2	Feature-based LiDAR SLAM - NDT
7.3.3	Feature-based LiDAR SLAM - LOAM: Plane and Edge
7.3.4	Challenges of LiDAR SLAM
7.3.5	Close-loop Constraints
7.4	Visual SLAM
7.4.1	Monocular Camera
7.4.2	Monocular SLAM
7.4.3	RGB-D camera
7.4.4	Challenges of a Visual SLAM
7.4.5	Integration of a Camera with LiDAR SLAM
7.5	Roles of IMU in LiDAR SLAMs
7.6	Conclusions
Appendix: Derivation of Epipolar Constraint
Appendix: List of Abbreviations
Bibliography
  
Introduction to SLAM
Map is a key element for indoor navigation. According to the Cambridge dictionary, a map is “a drawing that gives you a particular type of information about a particular area.” In other words, the composition of features associated with coordinate positions can be regarded as a map. Currently, most indoor mapping methods use simultaneous localization and mapping (SLAM) to build the first indoor map. This chapter will first introduce mapping methods using LiDAR, which can provide decimeter to centimeter accuracy in depth measurements to generate accurate 3D representations of indoor areas. However, the cost of LiDAR could hinder its applications in indoor navigation. In general, a camera is a low-cost option visually similar to the perception of our eyes. The second half of this chapter introduces visual SLAM for both monocular and depth cameras. The integration of LiDAR and camera is briefly discussed as a professional indoor mapping method that provides realistic 3D indoor maps for augmented reality applications. Finally, the roles of IMU in SLAM are discussed to provide readers with the core benefits of this integration. A flowchart of a general SLAM, which includes odometry and mapping, is shown in Figure 7.1. 
 
Figure 7.1: The generalized flowchart of SLAM.

7.1.1	Historical Development of SLAM
Simultaneous Localization and Mapping (SLAM) is a computational problem that involves constructing or updating a map of an unknown environment while simultaneously keeping track of an agent’s location within it. SLAM is fundamental to autonomous systems navigating in previously unexplored environments.
The SLAM problem was first conceptualized in the 1980s, with significant early contributions coming from researchers like Hugh Durrant-Whyte and John J. Leonard who built on the work of R. Smith, M. Self, and P. Cheeseman [1]. The initial theoretical foundations were established using EKF, marking the beginning of what we now call the classical era of SLAM research.
The development of SLAM can be broadly categorized into several eras:
	Classical Era (1986-2004): This period focused on establishing the theoretical foundations of SLAM. The research was dominated by filter-based approaches, particularly EKF-SLAM [1] and particle filters. A significant breakthrough came with FastSLAM [2], which used Rao-Blackwellized particle filters to efficiently solve SLAM by factoring the joint posterior into a product of a robot path posterior and landmark posteriors conditioned on the path.
	Algorithmic-Analysis Era (2004-2015): During this period, researchers focused on under- standing computational complexity, consistency, and convergence properties. Graph-based optimization methods emerged, and researchers began exploring how to handle large-scale environments and long-term operation. Key developments included Square Root SAM [3] and GraphSLAM [4].
	Robust-Perception Era (2015-Present): The current era is characterized by focusing on real- world robustness and adaptability. Modern SLAM systems are designed to work in dynamic, changing environments, with research emphasizing semantic understanding, object-level mapping, and deep learning integration. Key developments in this era include ORB-SLAM [5], LSD-SLAM [6], and LiDAR-based methods like LOAM [7] and LIO-SAM [8].
 
Figure 7.2: Evolution of SLAM technologies over time, showing key algorithms and their approximate relative impact.
Figure 7.2 provides a visual timeline of the evolution of SLAM technologies from 1990 to 2020. The figure illustrates the progression through three distinct eras, each represented by a different colored line. The Classical Era begins with EKF-SLAM in 1990 and extends to FastSLAM in 2002. The Algorithmic Analysis Era spans from Square Root SAM in 2004 to various developments including GraphSLAM and iSAM. The most recent Robust Perception Era includes modern approaches like LSD-SLAM, ORB-SLAM, VINS-Mono, and LIO-SAM. The vertical axis represents technological progress, showing how each era has built upon the foundations laid by previous research while introducing new paradigms and capabilities.

7.1.2	SLAM Frameworks and Evolution
The evolution of SLAM frameworks closely follows the hardware and algorithmic advancements in robotics and computer vision. Here, we provide an overview of major SLAM frameworks that have shaped the field.
GraphSLAM (2006): This approach formulated SLAM as a graph optimization problem. Poses and landmarks form the nodes of a graph, while measurements create edges (constraints) between nodes. The SLAM problem is then solved by finding the configuration of nodes that best satisfies all constraints [4]. GraphSLAM represented a significant paradigm shift from filter-based methods by recasting the entire SLAM problem into a sparse graph structure. By exploiting this sparsity, GraphSLAM achieves better computational efficiency for large-scale environments compared to earlier approaches. The method constructs information matrices where non-zero elements correspond to directly related poses and landmarks, allowing for efficient optimization using sparse linear algebra techniques. This graph-based representation also provides a natural framework for incorporating loop closures and handling uncertainty in a principled manner. GraphSLAM’s formulation has become foundational for many modern SLAM systems, as it scales well to large environments and complex scenarios by focusing computational resources on the most informative constraints.
PTAM (Parallel Tracking and Mapping, 2007): This introduced the idea of splitting tracking and mapping into separate threads, allowing real-time operation even with more computationally intensive mapping processes. PTAM was primarily designed for augmented reality applications but influenced many subsequent visual SLAM systems [9]. The key innovation of PTAM was recognizing that tracking camera motion could operate at frame rate with minimal computational resources, while the more complex task of map building and refinement could run asynchronously at lower frequencies. This parallel architecture enabled the use of computationally expensive bundle adjustment techniques in real-time applications for the first time. PTAM also introduced several practical innovations including a keyframe-based mapping approach, a map initialization procedure using a stereo initialization from a five-point algorithm, and an efficient relocalization system based on a modified version of randomized trees. By demonstrating that accurate camera tracking could be achieved in real-time on consumer hardware, PTAM sparked a revolution in visual SLAM research and applications, particularly in the AR/VR domain where low latency is critical.
LSD-SLAM (Large-Scale Direct Monocular SLAM, 2014): Unlike feature-based methods, LSD-SLAM directly works with image intensities, creating semi-dense depth maps. It efficiently handles scale drift and loop closures, making it suitable for large-scale environments [6]. As a direct method, LSD-SLAM represented a significant departure from the feature-based paradigm by operating directly on image intensities rather than extracted features. This approach allows it to exploit more of the available image information, particularly in texture-poor regions where feature detection often fails. LSD-SLAM builds semi-dense depth maps by estimating depths only at pixels with sufficient image gradient, striking a balance between computational efficiency and information utilization. One of its major contributions is a novel scale-aware image alignment algorithm that addresses the inherent scale ambiguity in monocular vision. LSD-SLAM also incorporates a pose graph optimization framework for global consistency and loop closure, allowing it to create large-scale maps with reduced drift. By demonstrating that direct methods could operate efficiently at scale, LSD-SLAM inspired subsequent research in direct and semi- direct SLAM approaches that seek to maximize the use of available visual information.
ORB-SLAM (2015): This feature-based system uses ORB (Oriented FAST and Rotated BRIEF) features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. Its robust design has made it one of the most popular visual SLAM systems, with versions for monocular, stereo, and RGB- D cameras [5]. ORB-SLAM represents the culmination of years of research in feature-based SLAM, integrating best practices and novel solutions into a comprehensive framework. The system employs a carefully designed three-thread architecture separating tracking, local mapping, and loop closing tasks. Its use of ORB features provides a good balance of computational efficiency and descriptive power, enabling real-time performance on modest hardware while maintaining robust feature matching across different viewpoints and lighting conditions. ORB-SLAM implements an efficient keyframe-based bundle adjustment approach for local map optimization, combined with a pose graph optimization for global consistency. The system also incorporates an innovative map initialization method that automatically detects either planar or general scenes, adapting its initialization strategy accordingly. These capabilities, combined with careful engineering for robustness, have made ORB-SLAM a standard baseline in the field and the foundation for numerous subsequent systems.
LOAM (Lidar Odometry and Mapping, 2014): This algorithm optimizes for low-drift and real-time performance without using GPS or IMU data. It extracts plane and edge features from point clouds and uses them for fast scan-to-scan odometry estimation and scan-to-map matching [7]. LOAM introduced a novel approach to LiDAR SLAM by decoupling the odometry and mapping processes into two parallel algorithms running at different frequencies. The high-frequency odometry algorithm extracts edge and planar features from the point cloud and matches them to the previous scan to estimate motion, while the lower-frequency mapping algorithm registers these features to a global map for refinement and loop closure. This dual-rate approach enables LOAM to achieve both real-time performance and high accuracy. LOAM’s feature extraction methodology is particularly innovative, using the distinctiveness of local surface geometry to classify points as edge or planar features, which reduces the computational burden of point cloud registration while maintaining sufficient constraints for accurate pose estimation. The system also incorporates distortion correction to account for sensor motion during scanning, which is crucial for accurate mapping during continuous movement. LOAM’s exceptional performance in the KITTI odometry benchmark demonstrated that LiDAR-only SLAM could achieve accuracy comparable to or exceeding systems using additional sensors like IMU or GPS, establishing it as a foundational approach for subsequent LiDAR SLAM research.
VINS-Mono (2018): A robust monocular visual-inertial state estimator. It tightly couples visual and inertial measurements to handle challenging scenarios like dynamic environments and fast motion [10]. VINS-Mono represents a significant advancement in visual-inertial fusion, addressing the limitations of vision-only systems while maintaining computational efficiency. The system employs a sliding window-based optimization framework that tightly integrates visual features and pre-integrated IMU measurements to estimate the full state including position, velocity, orientation, and sensor biases. VINS-Mono incorporates a robust initialization procedure that can reliably estimate initial states without requiring specific motions or known patterns. One of its key innovations is the incorporation of a 4-DOF pose graph for loop closure that maintains the gravity direction constraint from IMU, enabling more accurate global consistency. The system also features online spatial and temporal calibration capabilities, automatically determining the transformation between camera and IMU as well as time offsets between sensor measurements. VINS-Mono’s robust outlier rejection strategies and failure detection mechanisms make it particularly suitable for challenging real-world deployments where lighting conditions vary and dynamic objects are common. Its modular design and open-source implementation have made it a popular foundation for autonomous drone navigation, AR applications, and research in visual-inertial fusion.
LIO-SAM (2020): Builds on LOAM by tightly coupling LiDAR and IMU data using factor graph optimization. It incorporates IMU pre-integration and efficient loop closure to achieve high accuracy with real-time performance [8]. LIO-SAM addresses several key limitations of previous LiDAR SLAM systems through its innovative factor graph formulation and sensor fusion approach. Unlike loosely-coupled methods that use IMU only for initial state estimation, LIO-SAM fully integrates IMU constraints throughout the optimization process, enabling robust performance during rapid motion and featureless environments. The system employs a factor graph architecture with four types of factors: IMU pre- integration factors, odometry factors, GPS factors (when available), and loop closure factors. A key innovation is its dual-model design, maintaining two separate factor graphs — one for global mapping that preserves all constraints, and another for real-time odometry that is periodically reset to ensure computational efficiency. LIO-SAM also introduces scan context-based loop closure detection that leverages the 360° field of view of mechanical LiDARs for robust place recognition. The system’s ability to incorporate GNSS factors when available allows for georeferenced mapping, while still maintaining high accuracy in GNSS-denied environments. By achieving both high accuracy and real-time performance on modest hardware, LIO-SAM has enabled new applications in autonomous navigation, particularly for robots operating in complex 3D environments where robust localization is essential.
The progression from early filter-based approaches to modern optimization-based methods has been driven by several factors:
	Computational Resources: Modern computers allow for more complex algorithms and real-time processing of dense sensor data. The exponential growth in computing power, particularly in embedded systems and mobile processors, has enabled the deployment of sophisticated SLAM algorithms. The development of specialized hardware accelerators for visual processing and machine learning tasks has further expanded the capabilities of real-time SLAM systems.
	Sensor Technology: The development of affordable, high-quality sensors like RGB-D cameras and 3D LiDARs has enabled more accurate environment perception. The dramatic reduction in cost and size of 3D sensing technologies has been crucial for widespread SLAM adoption. Solid-state LiDARs have made high-precision depth sensing more accessible, while the mass production of RGB-D cameras initially driven by the gaming industry has provided affordable depth sensing for indoor applications. Time-of-flight cameras have improved depth accuracy in close-range applications, and event-based cameras with microsecond temporal resolution have opened new possibilities for high- speed motion tracking. Additionally, improvements in MEMS-based inertial sensors have made high-quality IMUs available at consumer price points, facilitating visual-inertial fusion approaches that enhance robustness and accuracy.
	Algorithmic Innovations: New optimization techniques, feature extraction methods, and loop closure strategies have improved SLAM robustness and accuracy. Mathematical breakthroughs in sparse optimization, manifold representation of rotations, and factor graph formulations have enabled more efficient and accurate state estimation. Learned feature descriptors leveraging deep neural networks have significantly improved feature matching robustness across varying conditions. Semantic SLAM approaches that incorporate object recognition and scene understanding are bridging the gap between geometric mapping and higher-level environmental representation. Novel outlier rejection techniques and robust estimation methods have enhanced performance in dynamic environments with moving objects. The incorporation of probabilistic approaches for handling uncertainty continues to improve reliability in challenging conditions, while recent advances in differentiable programming enable end-to-end optimization of entire SLAM pipelines.
Multi-Sensor Fusion: Modern SLAM systems often integrate data from multiple sensors (e.g., camera, LiDAR, IMU) to overcome the limitations of individual sensors. Complementary sensor characteristics can be leveraged to enhance robustness; cameras provide rich texture and color information, LiDARs offer precise depth measurements regardless of lighting, and IMUs provide high-frequency motion estimates resistant to visual degradation. Sophisticated fusion frameworks using factor graphs, Kalman filter variants, or deep learning approaches can optimally combine these heterogeneous data sources. Tight coupling of sensors at the measurement level, rather than simply fusing outputs, has proven particularly effective for handling challenging scenarios like rapid motion or feature-poor environments. Recent research has explored adaptive fusion strategies that dynamically adjust sensor weighting based on environmental conditions and sensor health, further improving reliability across diverse operating conditions. The integration of additional sensors like wheel encoders, barometers, magnetometers, and GNSS receivers provides even more constraints for robust state estimation in specific application domains.

7.1.3	Comparison of Popular SLAM Implementations
Different SLAM implementations have their strengths and weaknesses, making them suitable for different applications. Table 7.1 provides a comparison of some popular SLAM frameworks based on various aspects:
Table 7.1: Basic Comparison of Popular SLAM Implementations
Aspect	ORB-SLAM	LSD-SLAM	LOAM	VINS-Mono	LIO-SAM
Sensor Type	Camera (Mono/Stereo/ RGB-D)	Monocular -Camera	LiDAR	Monocular Camera + IMU	LiDAR + IMU
Feature Type	Sparse Features (ORB)	Semi-dense depth maps	Edge and planar features	Visual features (Optical flow)	Edge and planar features
Map Representation	Sparse point cloud	Semi-dense depth maps	Point cloud	Sparse point cloud	Point cloud
Localization Accuracy	High (with loop closure)	Medium	High	Very high	Very high
Computational Cost	Medium	Medium-High	Medium-High	High	Medium-High
Loop Closure	Yes	Yes	No (original version)	Yes	Yes
Scale Recovery	No (monocular)	No (monocular)	Yes	Yes (using IMU)	Yes
Real-time Performance	Yes	Yes (with GPU)	Yes	Yes	Yes
Environment Types	Well-textured	Well-textured	Structured environments	Various	Various
Major Advantages	Accurate, robust loop closure	Works in large environments	Good for unmanned vehicles	Robust to dynamic objects	High accuracy, leverages IMU
Major Limitations	Fails in textureless areas	Requires GPU for real-time	Motion distortion issues	Initialization challenges	Sensor synchronization requirements
Reference	[5]
[6]
[7]
[10]
[8]

Table 7.2 compares the pipeline components of LiDAR-based and Vision-based SLAM systems:
Table 7.2: Comparison of LiDAR-based and Vision-based SLAM Pipelines
Pipeline
Component	LiDAR SLAM	Visual SLAM
Data Preprocessing		Point cloud downsampling
	Motion distortion correction
	Ground/outlier removal		Image rectification
	Lens distortion correction
	Exposure adjustment
Feature Extraction		Edge features
	Planar features
	Corner points
	Normal distributions (NDT)		Corner features (FAST, Harris)
	Blob features (SIFT, SURF, ORB)
	Direct methods (photometric error)
Data Association		Nearest neighbor matching
	ICP matching
	Feature-to-feature matching		Feature descriptor matching
	Optical flow tracking
	Epipolar constraints
Motion Estimation		Point-to-point registration
	Point-to-plane registration
	NDT registration		Essential/Fundamental matrix
	Perspective-n-Point (PnP)
	Homography (for planar scenes)
Back-end Optimization		Factor graph optimization
	Pose graph optimization
	Bundle adjustment (with IMU)		Bundle adjustment
	Pose graph optimization
	Key-frame based optimization
Loop Closure		ICP-based scan matching
	Place recognition using point cloud
	Scan context matching		Bag of Words (BoW)
	DBoW2/DBoW3
	Visual place recognition
Map Representation		Point cloud map
	Surfel map
	Occupancy grid/voxel grid		Sparse point cloud
	Semi-dense depth maps
	Dense volumetric (TSDF)
Multi-sensor Fusion		LiDAR-IMU tight coupling
	LiDAR-Visual integration
	GNSS integration		Visual-Inertial fusion
	Multi-camera systems
	Visual-GNSS integration
Currently, the evolution of SLAM continues with active research in several directions:
	Semantic SLAM: Incorporating object-level and semantic understanding into SLAM systems [11].
	Deep Learning Integration: Using neural networks for feature extraction, depth estimation, and loop closure detection [12].
	Dynamic Environment Handling: Developing methods to handle moving objects and changing environments [13].
	Resource-constrained SLAM: Creating efficient algorithms for mobile and embedded devices [14].
	Event-based SLAM: Leveraging neuromorphic cameras (event cameras) for high-speed, high- dynamic-range visual SLAM [15].
These advancements are pushing SLAM systems toward greater autonomy, robustness, and applicability across diverse scenarios, from autonomous driving to AR/VR applications.
 

General Mathematical Model of SLAM
First, the features (which are the measurements used for the coming estimation, denoted by f ) are extracted from the raw data. If this is the first epoch, the device (i.e., camera and LiDAR)’s body local coordinate is used as the local map coordinate for the upcoming estimation, denoted by B in the superscript. Note that the features extracted at the first epoch are used as the initial map. The movement of an agent can be estimated by the transformation between features from adjacent epochs, incorporating a rotation C and a translation ∆x in 3-D space (from the subscript to the superscript coordinate systems). The feature transformation can be represented by the coordinate transformation, using
	f_(k,t)^(B_(t-1) )=C_(B_t)^(B_(t-1) ) f_(k,t)^(B_t)+〖∆x〗_(B_t)^(B_(t-1) )	(7.1)
where the superscripts B_t and B_(t-1) denote corresponding to the local body coordinate system at the epochs t and t-1, respectively. For multiple epochs, the transformation is usually described by matrix multiplication in a compact manner, as follows:
	[■(f_(k,t)^(B_(t-1) )@1)]=[■(C_(B_t)^(B_(t-1) )&〖∆x〗_(B_t)^(B_(t-1) )@0&1)][■(f_(k,t)^(B_t )@1)]	(7.2)
For convenience, the transformation is simply represented by the following multiplication, where the feature vector will be automatically extended with a row of 1 at the bottom during the multiplication.
	f_(k,t)^(B_(t-1) )=T_(B_t)^(B_(t-1) ) f_(k,t)^(B_t )	(7.3)
	T_(B_t)^(B_(t-1) )=[■(C_(B_t)^(B_(t-1) )&〖∆x〗_(B_t)^(B_(t-1) )@0&1)]	(7.4)
where T is the transformation matrix between the previous and current epoch of features is estimated, also known as a “pose” in robotics field. It is then used to transform a k-th feature from the current epoch of the device’s body coordinate, f_(k,t)^(B_t ), back to the k-th feature of the previous epoch of device’s body coordinate, f_(k,t)^(B_(t-1) ). Thus, the odometry can be obtained by optimizing T_(B_t)^(B_(t-1) ) to align the features from the previous epoch with those from the current one after the transformation, as below.
	T ̂_(B_t)^(B_(t-1) )=arg  min┬(T_(B_t)^(B_(t-1) ) )⁡∑_(k=1)^(N_feature)▒‖f_(k,t-1)^(B_(t-1) )-T_(B_t)^(B_(t-1) ) f_(k,t)^(B_t ) ‖^2 	(7.5)
For the mapping stage, the features at different epochs are all transformed to the local map coordinate to form the map M^L as below.
	M^L={M_t^L│∀t}	(7.6)
with M_t^L={f_(k,t)^L│k=1,⋯,N_feature } and f_(k,t)^L=T_(B_0)^L (∏_1^t▒T_(B_t)^(B_(t-1) ) ) f_(k,t)^(B_t ).
Then, both odometry and mapping are completed, meaning the SLAM is achieved. In fact, SLAM will make use of the fact that all the features are mutually observed in multi-epoch of the data. It means SLAM can further apply a batch optimization considering the co-visibility in a window containing several scans. The estimated transformation matrix and the map earlier are used as initial guesses for this batch optimization. The transformations in a window are batch optimized as follows.
	T ̂_window=(arg min)┬(T_window )⁡∑_(t=i)^(i+w)▒∑_(k=1)^(N_feature)▒‖M_window^L-T_(B_0)^L (∏_1^t▒T_(B_t)^(B_(t-1) ) ) f_(k,t)^(B_t ) ‖^2 	(7.7)
T_window={T_(B_t)^(B_(t-1) )│∀t∈[i,i+w] }
where i and i+w denote the index of the first and the last scan in the window respectively. M_window^L denotes the window map defined as {M_t^L│∀t∈[i,i+w] }. It’s worth mentioning that for the constraint construction of the certain pose T_(B_t)^(B_(t-1) ), the map points contributed by the corresponding scan M_t^L should be excluded from M_window^L during the match.
Finally, the batch optimized map can be obtained as M ̂_batch^L={M ̂_t^L│∀t}. M ̂_t^L is registered by the batch optimized transformation from (7.7). It’s different from (7.6) where the map is accumulated by the pose from odometry (7.5).
The maps built by these methods are mostly in a local coordinate. The connection between indoor maps to outdoor maps requires the coordination transformation from the local map coordinate to the world geodetic coordinate. If the transformation between the world coordinate and the local map coordinate is available (which is usually given by a GNSS receiver), the SLAM map can be transformed to the world coordinate to provide absolute map information.
	M^W=T_L^W M^L	(7.8)
Table 7.3 provides an overview of the relationship between measurements, model, and the state to be estimated. The table focuses on LiDAR and visual odometry, which are the primary methods discussed in this chapter. The formulas listed in the table will be detailed in the following sections.
Table 7.3: Comparison of LiDAR and visual odometry in terms of measurement, model and state.
Methods	Measurement	Model	State
LiDAR	p_(i,t)^(B_t )	p_(j,t-1)^(B_(t-1) )=C_(B_t)^(B_(t-1) ) p_(i,t)^(B_t )+〖∆x〗_(B_t)^(B_(t-1) )	T_(B_t)^(B_(t-1) )=[■(C_(B_t)^(B_(t-1) )&〖∆x〗_(B_t)^(B_(t-1) )@0&1)]
Monocular camera	p_(k,t+∆t)^I
p_(k,t)^I	(K^(-1) p_(k,t+∆t)^I )^T t^⋀ RK^(-1) p_(k,t)^I=0	T=[■(R&t@0&1)] and depth of features

 
LiDAR SLAM
A LiDAR sensor transmits pulsed light wave from a channel and receives a reflected wave that hits an object. There are two types of popular LiDAR; mechanical LiDAR and solid-state LiDAR. In general, the former one has a sparser point cloud data (PCD) but wider coverage of field of view (FoV) while the latter one has a dense PCD but narrower coverage of FoV. The distance between the LiDAR and object can therefore be determined by a two-way time-of-arrival (TOA) ranging method as shown in the left of Figure 7.4 If there are multiple channels, it is important to distinguish the reflection is coming from a specific channel, resulting in a sequence of binary codes (pulses) that has a characteristic of a low cross-correlation is usually used. Since it requires a few pulses to for the correlation, the LiDAR ranger usually has a requirement on the minimum distance between the sensor and an object. For a mechanical 3D LiDAR, a servomotor will rotate all the channels in 360 degrees to receive the reflections from its surrounding environment in 360 degrees, forming a point cloud as shown in the right of Figure 7.4. The point cloud is to describe as the distances between all the objects (which cause the reflections) and the LiDAR sensor and it is defined as follows.
	P_t=[■(p_1&⋯&p_i )]=[■(x_1&⋯&x_i@y_1&⋯&y_i@z_1&⋯&z_i )],i∈the received reflections	(7.9)
where the subscript i denotes the index of the point scanned in epoch t and (x_i,y_i,z_i) denotes the location of a point in the local body coordinate of the LiDAR at epoch t. The point cloud, P_t,  is therefore the measurement of LiDAR. The model is the surrounding environment, as shown in Figure 7.5. In the following section, a conventional point-based and two state-of-the-art feature-based LiDAR models are introduced. The challenges of LiDAR SLAM are also discussed [16].
 
Figure 7.3: Mechanical LiDAR sensing principle.

 
Figure 7.4: The measurements, models and estimation of using LiDAR point cloud data for odometry.

7.3.1	Point-cloud based LiDAR SLAM - ICP
The most well-known conventional method is iterative closest point (ICP). It minimizes the point-to- point distance between two scans of LiDAR PCD. The previous scan can be regarded as a model of surrounding environment and the current scan is regarded as the measurement. ICP can estimate the transformation matrix between the two scansT_(B_t)^(B_(t-1) ) when the minimum distance is achieved by iterations. It is described as follows.
	T ̂_(B_t)^(B_(t-1) )=arg  min┬(T_(B_t)^(B_(t-1) ) )⁡∑_(i=1)^(N_model)▒∑_(j=1)^(N_measurement)▒〖b_(i,j) ‖p_(i,t-1)^(B_(t-1) )-T_(B_t)^(B_(t-1) ) p_(j,t)^(B_t ) ‖〗^2 	(7.10)
where N_model and N_measurement denote the number of points in the previous and current scans, respectively. p_(i,t-1)^(B_(t-1) ) and p_(j,t)^(B_t ) are the points in the previous and current scans, respectively. b_(i,j) is a binary selector of the paired points, meaning that if p_(i,t-1)^(B_(t-1) ) and p_(j,t)^(B_t ) is a pair, then b_(i,j)=1 and otherwise b_(i,j)=0. To determine whether the two points in the two scans is a pair, generally speaking, a threshold, d_threshold, of the distance of the two points is used to make the judgement. 
	b_(i,j)={■(0,if ‖p_(i,t-1)^(B_(t-1) )-(T_(B_t)^(B_(t-1) ) )_0 p_(j,t)^(B_t ) ‖_2>d_threshold@1,otherwise)┤	(7.11)
where the subscript 0 for (T_(B_t)^(B_(t-1) ) ) denotes initial guesses. The value of the threshold is based on applications, for example, 1 meter is used for the application of LiDAR in an indoor parking lot. For a high (depending on the velocity of the LiDAR) scan rate of LiDAR, the null initial guess is fine. One way to obtain a better initial guess is to apply a constant translational and rotational velocities assumption to predict the transformation matrix of the next epoch of time. To solve (7.10), a nonlinear optimizer solver as mentioned Section 2.4. To reduce the computational load, the rotation matrix can be solved first by singular value decomposition (SVD), then the translation matrix can be solved by substituting C ̂_(B_t)^(B_(t-1) ) into 〖∆x ̂〗_(B_t)^(B_(t-1) )=p ̅_(t-1)^(B_(t-1) )-(C ̂_(B_t)^(B_(t-1) ) p ̅_t^(B_t ) ) [17]. The p ̅_(t-1)^(B_(t-1) ) and p ̅_t^(B_t ) denotes the geometric center of the points in the previous scan and the current scan respectively. As a scan of LiDAR PCD can easily contain more than 10,000 points using a typical mechanical 3D LiDAR, the computational load of the conventional ICP is high, meaning it could not be used in the applications that have limited computing resources. A straightforward idea to maintain the SLAM performance but to reduce the computational load is to maintain the observability of the measurement model but to reduce the number of points used. A voxel grid filter can be used to sampling the dense raw points [18]. Another solution is to exploit the representative features from the dense raw points where only partial of the distinct features are used for the point registration.
Finally, either a direct assembly of the PCD, e.g., (7.6), or a batch estimation considering all the PCD, e.g., (7.7), can be used to obtain the accumulated PCD map M={p_1^L⋯p_(N_point)^L } where N_point denotes the total number of the registered points.
To maintain the observability of the measurement model of the LiDAR SLAM, an idea is to extract “features” from the PCD. Two popular methods are 1) to represent the raw points inside a voxel by a normal distribution and 2) to extract 2D planes and 1D edges from the PCD.

7.3.2	Feature-based LiDAR SLAM - NDT
The normal distribution transform (NDT) method represents the points inside a voxel as a 3D normal distribution as shown in Figure 7.6 [19]. In other words, it describes the probability of finding a point in a certain position by a set of normal distributions [20].
 
Figure 7.6: An illustration of representing points as normal distributions.
For k-th voxel, the average p ̅_(k,t-1)^(B_(t-1) ) and covariance matrix Σ_(k,t-1)^(B_(t-1) ) of the points inside the voxel are calculated:
	p ̅_(k,t-1)^(B_(t-1) )=1/n_k  ∑_(i=1)^(n_k)▒p_(i,t-1)^(B_(t-1) ) 	(7.12)
	Σ_(k,t-1)^(B_(t-1) )=1/(n_k-1) ∑_(i=1)^(n_k)▒〖(p_(i,t-1)^(B_(t-1) )-p ̅_(k,t-1)^(B_(t-1) ))〖(p_(i,t-1)^(B_(t-1) )-p ̅_(k,t-1)^(B_(t-1) ))〗^T 〗	(7.13)
where n_k denotes the total number of points inside the k-th voxel. To apply these normal distribution features in SLAM, the transformation matrix T_(B_t)^(B_(t-1) ) is associated with the NDT measurement model. Comparing to ICP, the NDT method models the surrounding environment by voxels that represented by normal distributions. If the points in the current scan can achieve a maximum joint probability with a given transformation matrix T ̂_(B_t)^(B_(t-1) ), then the given T ̂_(B_t)^(B_(t-1) ) is solution with the maximum likelihood. The likelihood (which can be regarded as a score) of voxel k referring to T_(B_t)^(B_(t-1) ), can be described as:
	likelihood_k (T_(B_t)^(B_(t-1) ) )=∏_(j=1)^(N_measurement)▒〖exp(-(‖T_(B_t)^(B_(t-1) ) p_(j,t)^(B_t )-p ̅_(k,t-1)^(B_(t-1) ) ‖_(Σ_(k,t-1)^(B_(t-1) ))^2)/2)〗	(7.14)
where ‖r‖_Σ^2 denotes r^T Σ^(-1) r. Considering all the voxel, the joint likelihood becomes:
	likelihood(T_(B_t)^(B_(t-1) ) )=∏_(k=1)^(N_voxel)▒∏_(j=1)^(N_measurement)▒〖exp(-(‖T_(B_t)^(B_(t-1) ) p_(j,t)^(B_t )-p ̅_(k,t-1)^(B_(t-1) ) ‖_(Σ_(k,t-1)^(B_(t-1) ))^2)/2)〗	(7.15)
Since the likelihood is Gaussian distributed, the MLE of (7.15) can be estimated by a nonlinear least squares estimation.
	T ̂_(B_t)^(B_(t-1) )=arg min┬(T_(B_t)^(B_(t-1) ) )  1/2 ∑_(k=1)^(N_voxel)▒∑_(j=1)^(N_measurement)▒‖T_(B_t)^(B_(t-1) ) p_(j,t)^(B_t )-p ̅_(k,t-1)^(B_(t-1) ) ‖_(Σ_(k,t-1)^(B_(t-1) ))^2 	(7.16)
The NDT method divides the surrounding environment into fix-sized cells. This may lead to dis- continuities in the model representation since some voxel may contain very little points. The insufficient constraints could be problematic in calculating the Jacobian matrix of the NDT measurement model [21]. A method that combines the ideas of ICP and NDT is proposed and called Generalized-ICP [22]. It firstly associates a group of points between the previous and current scans, then models the group of points by normal distributions and finally does the distribution-to-distribution matching.

7.3.3	Feature-based LiDAR SLAM - LOAM: Plane and Edge
A state-of-the-art LiDAR method is LOAM, LiDAR Odometry and Mapping [7]. Its aims are 1) to maintain a low-drift of the mapped PCD map and 2) to reduce the computational load of SLAM. In the previously mentioned ICP and NDT, the two SLAMs are matching the previous and current scans, i.e., a scan-to-scan matching. These scan-to-scan matchings only consider the points in the vicinity of the two scans, meaning the points (features) at far are not really used in the calculation of the transformation, see (7.10) & (7.16). This results in drifting to the conventional ICP and NDT maps. LOAM proposed a scan-to-“map” approach (refer to the dash line in the upper right of Figure 7.1). The map means a locally registered map during the SLAM process. This local map is the aggregated PCD of all previous scans so that it also considers the points (features) at far. This scan-to-map approach can effectively reduce the drift. However, this approach drastically increases the computational load, hence the LOAM has to reduce the computational load at the same time. To achieve this aim, LOAM extracts representative edge and planar features from PCD to reduce the redundancy of the dense point clouds while maintaining the model observability. The extraction accords to the curvature of a few segmented points in a current scan Pt. A point is selected as an edge point if its curvature value is larger than a pre-determined threshold, or classified as a planar point by a smaller curvature, as Figure 7.7 shows.
 
Figure 7.7: Illustration of extracted edge (black points) and planar points (grey boxes) for scan-to-map concerning a frame of LiDAR point clouds (grey points)
To further reduce the computation load of the SLAM, the lower update rate of the scan-to-map approach is desired. This reduction of the update rate results in another challenge; a more stringent requirement on the quality of the initial guess comparing to a scan-to-scan approach. In LOAM, a scan-to-scan odometry is used to provide that initial guess for the later scan-to-map matching. Thus, it is a two-step approach. (7.17) describes the rough transformation estimated by the odometry.
	T ̂_odom=arg min┬(T_odom )   1/2{∑_(k=1)^(N^edge)▒‖ω_((k)) f (p_((k,t))^edge,T_odom)‖^2 +∑_(k=1)^(N^plane)▒‖ω_((k)) f(p_((k,t))^plane,T_odom)‖^2 }	(7.17)
where ω_((k)) denotes the bisquare weight, setting as 1-1.8×f(p_((k,t) ),T_odom ) [7]. f(p_((k,t))^edge,T_odom ) denotes a point-to-line distance which is calculated as following.
	f(p_((k,t))^edge,T_odom )=‖(T_odom p_((k,t))^edge-p_((j,t-1))^edge )×(T_odom p_((k,t))^edge-p_((l,t-1))^edge)‖_2/‖p_((j,t-1))^edge-p_((l,t-1))^edge ‖_2 	(7.18)
where p_((k,t))^edge denotes the k-th edge feature points on the current scan. p_((j,t-1))^edge and p_((l,t-1))^edge denote two closest edge features of p_((k,t))^edge in the previous scan which are assumed be on the corresponding line segment of p_((k,t))^edge. As shown in Figure 7.8, p_((j,t-1))^edge and p_((l,t-1))^edge are selected from adjacent rings so that vertical lines orthogonal to the laser beam are exploited. Such lines are free from motion distortion compared with that parallel to the laser beam.
 
Figure 7.8: Illustration of point-to-line distance (left) and point-to-plane (right) distance for scan-to-scan odometry.
In (7.19), f(p_((k,t))^plane,T_odom) is formulated as the point-to-plane distance for planar feature points as follows,
	f(p_((k,t))^plane,T_odom )=‖█((T_odom p_((k,t))^plane-p_((j,t-1))^plane )@((p_((j,t-1))^plane-p_((l,t-1))^plane )×(p_((j,t-1))^plane-p_((m,t-1))^plane )) )‖_2/‖(p_((j,t-1))^plane-p_((l,t-1))^plane )×(p_((j,t-1))^plane-p_((m,t-1))^plane )‖_2 	(7.19)
where p_((k,t))^plane denotes the k-th planar feature points on the current scan. p_((j,t-1))^plane, p_((l,t-1))^plane, and p_((m,t-1))^plane denotes the three closest planar feature points of p_((k,t))^plane in the previous scan which are assumed be on the corresponding planar patch of p_((k,t))^plane. As shown in Figure 7.8, p_((j,t-1))^plane and p_((l,t-1))^plane are selected from the same ring while p_((m,t-1))^plane are selected from an adjacent ring. Consequently, the three points are not colinear to form a planar patch. 
Then, the rough transformation T ̂_odometry is used to estimate a refined transformation by the scan-to-map approach. The objective function is formulated as (7.17). However, the point-to-line (plane) distance is that between p_((k,t))^edge  (p_((k,t))^plane) and the line (plane) fitted by its closest points selected from the map rather the previous scan.
Finally, the refined transformation will be used in the batch optimization as mentioned in Eq. (7.7). 

7.3.4	Challenges of LiDAR SLAM
For IPIN applications, the adverse weather conditions are not a problem for LiDAR SLAM. The two major challenges are the motion distortion and the presence of dynamic objects.
Motion distortion
Ideally speaking, LiDAR SLAM assumes all the points are scanned at the same location of the ego- vehicle. In reality, the movement of the ego-vehicle while the LiDAR is scanning will cause a motion distortion to the scanned LiDAR PCD. This phenomenon is illustrated as Figure 7.9 and Figure 7.10. As can be seen when LiDAR stays static as the blue dot in the left part of Figure 7.9, the scanning pattern remains as a circle. While when LiDAR moves from the blue dot to the green dot like in the right part of Figure 7.9, the scanning pattern is distorted. This is a factor resulting in the drift of accumulated LiDAR PCD map. In fact, this distortion can be compensated if the rotation rate and acceleration of the ego-vehicle is roughly known. A popular method is to integrate with IMU to provide such measurements to compensate the distortion [23]. The poses of the LiDAR during the scan sweep are predicted. As shown in the following equation, each point is transformed to the starting moment of the scan so that the motion distortion is compensated.
	p ̃_i=T_i p_i	(7.20)
where p_i denotes the i-th point produced by the LiDAR during a scan sweep at the LiDAR body frame. T_i is the LiDAR pose currently related to scan starting time. p ̃_i denotes the point with distortion compensation. T_i is predicted as below,
	T_i=[■(R_i&t_i@0&1)]=[■(exp(ω∙∆t_i )^∧&v∙∆t_i@0&1)]	(7.21)
where ∆t_i denotes the time increment when p_i is achieved relative to the scan starting point. ω and v is the average rotation rate and average velocity provided by IMU during ∆t_i respectively. (⋅)^∧  represents the skew symmetry matrix of a vector which transforms the rotation vector to the rotation matrix with further exponential transformation.
 
Figure 7.9: Illustration of motion distortion due to LiDAR movement.

 
Figure 7.10: Motion distortion before (left) and after (right) applying correction.
Dynamic objects
A major assumption made in LiDAR SLAM is that the surrounding environment is static, meaning there are only still objects. In other words, LiDAR SLAM assumes that the change of the distances between the ego-vehicle to the surrounding objects is due to the relative transformation of ego-vehicle the between two epochs. As Figure 7.11 shows, the position change of the dynamic object is not due to the position change of the ego-vehicle, resulting in the estimated relative transformation of ego-vehicle is not accurate. This is particularly a problem when LiDAR SLAM is used in urban areas where the number of dynamic objects is excessive [24].
 
Figure 7.11: LiDAR odometry without (left) and with (right) the impact of a dynamic object.
There are several approaches to mitigate the effect. One of the intuitive approaches is to detect the dynamic object using advanced object recognition approach [25] to exclude the point cloud that came from the dynamic object before its use in LiDAR SLAM. Another approach is to apply a robust estimator (such as the ones introduced in Section 2.3) to mitigate the impact of these outlier measurements [26].
7.3.5	Close-loop Constraints
A loop closure should be detected once the sensor carrier arrives at the same place it has been. A Euclidean distance-based loop closure detection approach is proposed [8]. For the latest estimated pose T_j, the closest prior pose T_i is searched in the surrounding Euclidean space with certain size. T_i is selected as the loop pair for T_j if the Euclidean distance between them is smaller than a pre-determined threshold. The transformation ∆T_ij^' between the loop pair is achieved by matching the i scan with the sub-map composed of scans around the j one. The close-loop constraint is formulated as the residual between the observed pose increment ∆T_ij^' and the practical one derived from the state as follows,
	f(T_i,T_j,∆T_ij^')=〖ln(〖∆T_ij^'〗^(-1) 〖T_i〗^(-1) T_j )〗^∨	(7.22)
For implementation feasibility (e.g. derivative calculation of the residual relative to the state), the residual is transformed from the matrix format to the Lie Algebra format by 〖ln(⋅)〗^∨. ln(⋅) transforms the transformation matrix to the skew-symmetric matrix constructed from its corresponding Lie algebra. (∙)^∨ recover the Lie algebra from its skew symmetric matrix.

 
Visual SLAM
There are two popular types of cameras that are used for visual SLAM for IPIN applications; monocular and RGB-D cameras, which will be introduced in the following sub-sections. Stereo camera based SLAM is not introduced in the book for the following reasons. The stereo camera based approach using the baseline between the left and right cameras to calculate the disparity to reconstruct the depth of the images. However, the baseline is usually very short, e.g., the stereo camera on smartphones, resulting in the reconstructed depth is inaccurate for objects at long distances. The poor depth measurements limit its performance in odometry and SLAM. As a result, a monocular camera based SLAM is usually used even when a stereo camera is available. In other words, only left or right camera is used. As a result, the monocular camera based SLAM is still the most popular approach.
7.4.1	Monocular Camera
Comparing to LiDAR, a monocular camera cannot directly measure the distance (also well known as “depth” in image processing) between the sensed objects and the camera. To use a monocular camera in a visual odometry, it has to use a sequence of 2D images (t and t + 1) to estimate the transformation matrix of the camera pose between two epochs of time. In other words, the change of a same feature in two images can be used to infer the change of the camera pose as shown in Figure 7.15. The overall flow of a visual SLAM can be simplified as Figure 7.16, which each of the functions will be introduced in the following sub-section.
 
Figure 7.15: Illustration of the idea of estimating a transformation matrix using the detected and tracking features on a sequence of images.
 
Figure 7.16: The measurements, models and estimation of using an image for a monocular camera based odometry.
Feature Detection and Tracking
As the visual odometry makes use of the change of a same feature in two images to infer the change of the camera pose, it is utterly important to detect distinctive features from an image. Obviously, corner points are much more distinctive than lines since it can be easily identified in a 2D image by our eyes (See Figure 7. 17).
 
Figure 7. 17: Illustration of corner points are distinctive in a 2D plane.
The corner point that can be detected by a widely-applied method, Shi-Tomasi method [27]. Its idea is to check whether there are two lines in a searched area of the image. First, the image is transformed from RGB to a grayscale image for making only one value in each pixel. Then, a 2D window with the size of w_u and w_v in u- and v-axes is used as a region of interest (ROI) to find corner points, as shown in Figure 7.18.
 
Figure 7.18: (Left) Illustration of a 2D window used as a ROI to the searched corner points. (Right) Associated eigenvalues when there is either a line or a corner in the window.

The goal here is to find the corner point, meaning the intersection of two lines. Thus, the representation of the lines using the changes of grayscale values insides the window is important. The grayscale change Ep(up, vp) within the ROI of the searched pixel can be calculated as below.
	E_p (u_p,v_p )=∑_(w_u)▒∑_(w_v)▒〖W(w_u,w_v)[I(u_p+w_u,〖v_p+w〗_v )-I(u_p,v_p)]〗^2 	(7.23)
where I denotes a grayscale image. u_p  and v_p denote the location of the pixel index p used to search the corner points, which is the center of the searching window. I(u_p,v_p) denotes a specific grayscale value. W(w_u,w_v) is the weighting of the corresponding pixel within this window. The calculation of (7.23) can be regarded as a weighted convolution process between the I(u_p,v_p) and the surrounding grayscale value in this window. In general, the window will be set as a quadratic Gaussian distribution with the window center as the origin.
By linearizing the change of grayscale values in the window using a 1st order Tayler series expansion, the below equation can be derived.
	I(u_p+w_u,〖v_p+w〗_v )≈I(u_p,v_p )+I_(w_u ) w_u+I_(w_v ) w_v	(7.24)
where I_(w_u )=∂I(u_p,v_p )/(∂u_p ) and I_(w_v )=∂I(u_p,v_p )/(∂v_p ). I_(w_u ) and I_(w_v ) are the gradient of the gray value changes according to pixel location directions in u and v, respectively, i.e., the partial differential of I. By substituting (7.24) back to (7.23), the grayscale change within the window can be modelled as below. 
	E_p (u_p,v_p )≈[■(u_p&v_p )]G[■(u_p@v_p )]	(7.25)
	G=∑_(w_u)▒∑_(w_v)▒〖W(w_u,w_v)[■(I_(w_u)^2&I_(w_u ) I_(w_v )@I_(w_v ) I_(w_u )&I_(w_v)^2 )]=U^(-1) [■(λ_(p,1)&0@0&λ_(p,2) )]U〗	(7.26)
 
If there is a corner point in the window, both the eigenvalues in the derived 2D model should be larger than a certain value as shown in the right of Figure 7.18. Note that an eigenvector of a model is a nonzero vector that changes at most when the model is applied to by a state value. Eigenvalues are then used to indicate how much the vectors are scaled. In this example, the vectors can be understood as the major lines in the window. Thus, a SVD is applied in the model to obtain the eigenvalues, λ_(p,1) and λ_(p,2). If both the eigenvalues are larger than a pre-defined threshold, λ_min. Then, it is concluded that there is a corner point in this window. All the pixels p in the image are scanned to find all the detected corner points f. For the example in Figure 7.18 with a uniform weighting W within the window, the detection of features (i.e., corner points) is achieved by
	∑_(w_u)▒∑_(w_v)▒[■(I_(w_u)^2&I_(w_u ) I_(w_v )@I_(w_v ) I_(w_u )&I_(w_v)^2 )] =U^(-1) [■(λ_(p,1)&0@0&λ_(p,2) )]U	(7.27)
	f={p│λ_(p,1)>λ_min,&λ_(p,2)>λ_min }	(7.28) 
 
 
As mentioned in Figure 7.15, the key of a visual odometry to use a 2D-correspondance of images to infer the change of 3D pose (3D position and 3D orientation) of a camera. A popular and computational efficient method to obtain the 2D-correspondance of the detected features is Optical Flow with Lucas-Kanade method [28]. Its idea is as following and in Figure 7.19. 
 
Figure 7. 19: Illustration of searching whether the detected feature k in the current epoch exists in the image of the next epoch.
For each k-th detected feature in the image of the current epoch, located at [■(u_k&v_k )]^T∈f, a window in the image of the current epoch is used to check whether the feature is still existed in the image of the next epoch. Note that the window here is used to track feature movements, which is different from the ROI window in (7.23) for detecting features. If the grayscale value changes dramatically over a threshold ε_grayscale, it is assumed that the detected feature is no longer exists in the current image. It should be noted if the brightness of the image change may cause the loss of detected features, which is one of the reasons that monocular-camera-based visual odometry, is less robust. 
	ε_k=∑_(w_u)▒∑_(w_v)▒(I_t (u_k,v_k )-I_(t+∆t) (u_k+w_u,v_k+w_v ))^2 	(7.29)
	f_track={[■(u_k&v_k )]^T│ε_k<ε_grayscale }	(7.30)
The next step is to derive the pixel change ∆u_k and ∆v_k of a tracked feature k in f_track. Note that the pixel change is the 2D correspondence that is used as a measurement mentioned in Figure 7.16. If the constant brightness assumption can be made, then for each k, the grayscale value of the tracked features in the images of the previous and current epochs are remained the same. 
	I(u_k,v_k,t)=I(u_k+∆u_k,v_k+∆v_k,t+∆t)	(7.31)
By applying the 1st order of Taylor series expansion, the following equation can be derived. Here, the change of grayscale value is assumed to be linear. 
	I(u_k+∆u_k,v_k+∆v_k,t+∆t)≈I(u_k,v_k,t)+∂I/(∂u_k ) ∆u_k+∂I/(∂v_k ) ∆v_k+∂I/∂t ∆t	(7.32)
If another assumption that the detected feature did not move in the real world is made (static feature), where the movement of ∆u_k and ∆v_k are due to camera movement, the following equations can be obtained. 
	∂I/(∂u_k ) ∆u_k+∂I/(∂v_k ) ∆v_k+∂I/∂t ∆t=0	(7.33)
	∂I/(∂u_k )  (∆u_k)/∆t+∂I/(∂v_k )  (∆v_k)/∆t=-∂I/∂t	(7.34)
	[■(I_(u_k )&I_(v_k ) )][■(u ̇_k@v ̇_k )]=[-I_t ]	(7.35)
where u ̇_k and v ̇_k are the velocity of the tracked feature at the u and v directions, respectively. 
Finally, another assumption that all the pixels in a searched window in the current epoch of image are spatially consistent is made. For example, if a static object is captured inside the window, then this assumption is valid. This assumption means that all the pixels in the search window based on the k-th tracked feature are used to calculate the velocity of the pixel of the k-th feature tracked. The benefit is to provide more measurements to estimate the velocity of a tracked feature comparing to just using a tracked pixel to a tracked pixel of the two epochs. In other words, the estimation of the pixel change of a tracked feature k becomes an over-determined system. The matrix form below can be obtained as shown in the equation.
	[■(I_(u,1)&I_(v,1)@I_(u,2)&I_(v,2)@■(⋮@I_(u,(w_u×w_v)) )&■(⋮@I_(v,(w_u×w_v)) ))][■(u ̇_k@v ̇_k )]=-[■(I_(t,1)@I_(t,2)@■(⋮@I_(t,(w_u×w_v)) ))]	(7.36)
H_k x_k=-z_k
x ̂_k=(H_k^T H_k )^(-1) H_k^T (-z_k)
	k∈f_track, feature tracked
An example of detected and tracked visual features by the methods mentioned above is shown in Figure 7.20. The white lines are the measured 2D-correspondances for the estimation of the camera pose. It should be noted the accuracy and robustness of feature detections and tracking would greatly determine the performance of visual odometry and SLAM. ORB is a popular and a patent-free method to combine the features as a descriptor to improve the robustness and accuracy of the estimation of  the 2D-correspances [29]. A comparison of the different feature tracking methods can be found [30]. Recently deep learning networks are used to detect and track the features [31], which may relax the three assumptions, meaning 1) the constant brightness between two epochs, 2) the track features remain static in two epochs, and 3) a spatial consistency of the window searched, made in the Lucas-Kanade method. 
 
Figure 7.20: Example of feature detection and tracking.
Camera Model
Pinhole model for generating an image: To estimate the pose of the camera, three coordinate systems are involved: the pixel plane (2D), imaging plane (2D) and the camera-body coordinate (3D). Figure 7. 21 illustrates a pinhole imaging model. If there is an object located at p^C=[■(x^C&y^C&z^C )]^T in the camera-body coordinate, by this model its position in imaging plane becomes [■(x^I&y^I )]^T. According to the triangular similarity, they have a relationship,
	z^C/f=x^C/x^I =y^C/y^I 	(7.37)
where f denotes the focal length of the camera. As previously mentioned, an image is usually described by pixels, meaning a pixel plane. The position of a pixel p^pixel=[■(u&v)]^T can be obtained by applying scaling (θ_x and θ_y) and translation (c_x and c_y) to the object in the image plane, which is obtained by another scaling with the focal length from the camera-body coordinate, as below. 
	u=θ_x x^I+c_x=(θ_x fx^C)/z^C +c_x=(f_x x^C)/z^C +c_x	(7.38)
	v=θ_y y^I+c_y=(θ_y fx^C)/z^C +c_x=(f_y y^C)/z^C +c_y	(7.39)
By reorganizing (7.38) and (7.39) in a matrix form related to the camera-body coordinate,
	z^C [■(u@v@1)]=[■(f_x&0&c_x@0&f_y&c_y@0&0&1)][■(x^C@y^C@z^C )]	(7.40)
sp^pixel=Kp^C
where K denotes an intrinsic parameter matrix. f_x and f_y are the focal length in x- and y-axes of the imaging plane incorporating the pixel-image resolution scaling, respectively. [c_x,c_y ]^T is the principal point, and s=z^C denotes the depth of the object. It is clear that the object in the physical world became mathematically related to the pixel in an image. Generally speaking, the intrinsic parameter matrix only needs to be calibrated once before the camera is used to apply visual odometry.
 
Figure 7. 21: The planes and coordinate related a pinhole imaging model.
Distortion: In visual SLAM, the camera distortion is not negligible. As shown in Figure 7.22, it is caused by both the lens distortion and the assembly angle between the camera sensor and vertical plane. It can divide into radial and tangential distortions (Figure 7.22). These distortions can be corrected by the following model [32],
	{■((x^I ) ̂=x^I (1+k_1 r^2+k_2 r^4+k_3 r^6+2p_1 x^I y^I+p_2 〖(r〗^2+2〖x^I〗^2)@(y^I ) ̂=y^I (1+k_1 r^2+k_2 r^4+k_3 r^6+2p_2 x^I y^I+p_1 〖(r〗^2+2〖y^I〗^2))┤	(7.41)
where, (k_1,k_2,k_3) and (d_1,d_2) are the coefficients of the radial and tangential distortions, respectively. (x^I,y^I) is the ideal position on imaging plane (without distortion), ((x^I ) ̂,(y^I ) ̂) is the real distorted position. With the intrinsic parameters, the distortion parameters can be obtained by camera calibration. Based on this undistorted model, the true projection position p^pixel=[u,v] of a point on the pixel plane is
	u=f_(x^I ) x ̂^I+c_(x^I )	(7.42)
	v=f_(y^I ) y ̂^I+c_(y^I )	(7.43)
 
Figure 7.22: Illustration of the camera radial and tangential distortions.
Visual Odometry
The relationship between the two consecutive camera poses (two epochs) and detected features in the images can be described using Figure 7.23. This is known as an epipolar geometry [33].
The feature points p_(k,t)^(pixel_t ) and p_(k,t+∆t)^(pixel_(t+∆t) ) are a matching pair in the two images and they correspond to the same 3D point p_(k,t)^(C_t ). The camera centers at two adjacent epochs and the 3D point p_(k,t)^(C_t ) can determine a plane, called epipolar plane. The line between these two camera centers is the baseline. The intersections of the baseline and the image planes at two adjacent epochs are epipoles, e_(k,t)  and e_(k,t+∆t). The lines l_(k,t) and l_(k,t+∆t) between a pixel and the epipole are called epipolar line. The pose of the k-th detected feature in camera-body frame at time t+∆t can be obtained from the pose at t by applying a rotation matrix R and a translation vector t from the camera-body coordinate C_t to C_(t+∆t), as follows.
 
Figure 7.23: Illustration of epipolar geometry with features between adjacent epochs.
	p_(k,t+∆t)^(C_(t+∆t) )=Rp_(k,t)^(C_t )  +t	(7.44)
where the rotation and translation can be described by a transformation matrix as (7.4). In the pixel-frame, the detected feature k at time t and t+∆t can be expressed as below.
	p_(k,t+∆t)^pixel=p_(k,t)^pixel+[■(u ̇_k@v ̇_k )]∆t, k∈f_track, feature tracked	(7.45)
where [■(u ̇_k@v ̇_k )] is the measurement that is obtained from features detection and tracking by (7.36). Then, (7.40) is substituted by (7.44).
	s_(k,t+∆t) p_(k,t+∆t)^pixel=K(Rp_(k,t)^(C_t )+t)	(7.46)
Note that the depth of the k-th feature is changing with time. By using an epipolar constraint, p_(k,t)^(C_t )∙(t×p_(k,t)^(C_t ) )=0, which means the inner product of the vector, p_(k,t)^(C_t ), and the normal vector of an epipolar plane for detected feature k,  (t×p_(k,t)^(C_t ) ), is zero, the following equation can be obtained [34]. The derivation can be found at Appendix 7.A.
	(K^(-1) p_(k,t+∆t)^pixel )^T t^⋀ RK^(-1) p_(k,t)^pixel=0	(7.47)
where the operator ^ denotes the skew-symmetric matrix of a vector, as follows:
	t=[■(t_x@t_y@t_z )],t^⋀=[■(0&-t_z&t_y@t_z&0&-t_x@-t_y&t_x&0)]	(7.48)
Then, an essential matrix E is defined as following.
	E=t^⋀ R=[■(e_1&e_2&e_3@e_4&e_5&e_6@e_7&e_8&e_9 )]	(7.49)
	[K^(-1) p_(k,t+∆t)^pixel ]^T [■(e_1&e_2&e_3@e_4&e_5&e_6@e_7&e_8&e_9 )][K^(-1) p_(k,t)^pixel ]=0	(7.50)
As can be seen here, by finding eight pairs of matching feature points between two frames [35]. The exact solution of the fundamental matrix can be solved. Since the tracked feature might contain random noise, it is common to use more than eight pairs of features to find the elements in the fundamental matrix, meaning a least square estimation can be used. Since the translation matrix is t^⋀ is a skew-symmetric matrix and R is an orthogonal matrix, the SVD of the essential matrix E can decouple t^⋀ and R as shown in the equations below [36].
	E=UΣV^T	(7.51)
	t^⋀=[■(t_1^^&t_2^^ )], R=[■(R_1&R_2 )]	(7.52)
	t_1^^=UR_z (+90^o )ΣU^T	(7.53)
	t_2^^=UR_z (-90^o )ΣU^T	(7.54)
	R_1=UR_z^T (+90^o ) V^T	(7.55)
	R_2=UR_z^T (-90^o ) V^T	(7.56)
	R_z (θ)=[■(cos⁡θ&-sin⁡θ&0@sin⁡θ&cos⁡θ&0@0&0&1)]	(7.57)
Finally, the transformation can be obtained, meaning the visual odometry is achieved. After solving R_1, R_2, t_1^^ and t_2^^, four combinations of transformation solution Rp_(k,t)^(C_t )  +t can be obtained. Note that the solution is valid only when the associated depth of p^C is positive in camera-body frame at time t and t+∆t. Thus, the unique solution of the transformation matrix can be selected from the above four solutions.

7.4.2	Monocular SLAM
In monocular SLAM, an inherent challenge is always present, called scale uncertainty. This problem refers to the real size of the object in the world frame is unknown. Briefly, if we scale down or up the motion of camera as well as the scene size by a certain multiple, we will capture the same images. Thus, the trajectory estimated by the monocular SLAM will differ from the real trajectory by a scale.
According to the visual odometry, the transformation T between the two consecutive camera poses can be obtained by (7.51-7.57). Then, the positions of the features in the 3D space need to be calculated by the pose estimation results of camera. For the point with the 3D position p_(k,t)^(C_t ) in Figure 7.23, according to the camera projection model in (7.44), the two projected pixel positions p_(k,t)^I,p_(k,t+∆t)^I in the two images can be obtained by feature detection, we have
	s_(k,t) p_(k,t)^I=Kp_(k,t)^(C_t )	(7.58)
	s_(k,t+∆t) p_(k,t+∆t)^I=K(Rp_(k,t)^(C_t )+t)	(7.59)
By rearranging the equation:
	K^(-1) s_(k,t) p_(k,t)^I=p_(k,t)^(C_t )	(7.60)
	K^(-1) s_(k,t+∆t) p_(k,t+∆t)^I=Rp_(k,t)^(C_t )+t	(7.61)
By substituting (7.60) into (7.61):
	s_(k,t+∆t) K^(-1) p_(k,t+∆t)^I=s_(k,t) RK^(-1) p_(k,t)^I+t	(7.62)
	s_(k,t+∆t) q_(k,t+∆t)^(C_t )=s_(k,t) Rq_(k,t)^(C_t )+t	(7.63)
where q_(k,t+∆t)^(C_t )=K^(-1) p_(k,t+∆t)^I and q_(k,t)^(C_t )=K^(-1) p_(k,t)^I are the normalized coordinates of 3D point p_(k,t)^(C_t ) with Z=1 in the two camera frames.
In (7.63), there are two unknowns, s_(k,t+∆t) and s_(k,t), To solve this, we can use the cross product with q_(k,t+∆t)^(C_t ) on both sides of the equation:
	q_(k,t+∆t)^(C_t )×(s_(k,t+∆t) q_(k,t+∆t)^(C_t ) )=q_(k,t+∆t)^(C_t )×(s_(k,t) 〖Rq〗_(k,t)^(C_t )+t)	(7.64)
Since any vector crossed with itself equals zero, the left side becomes zero:
	0=q_(k,t+∆t)^(C_t )×(s_(k,t+∆t) q_(k,t+∆t)^(C_t ) )	(7.65)
	0=s_(k,t) (q_(k,t+∆t)^(C_t )×〖Rq〗_(k,t)^(C_t ) )+q_(k,t+∆t)^(C_t )×t	(7.66)
Rearranging to isolate s_(k,t):
	s_(k,t) (q_(k,t+∆t)^(C_t )×〖Rq〗_(k,t)^(C_t ) )=-(q_(k,t+∆t)^(C_t )×t)	(7.67)
In (7.67), R,t can be solved by (7.51-7.57). q_(k,t)^(C_t ) and q_(k,t+∆t)^(C_t ) are obtained by their definition. Thus, s_(k,t) can be solved, and then substitute the solved s_(k,t) into s_(k,t) p_(k,t)^I=Kp_(k,t)^(C_t ) to obtain p_(k,t)^(C_t ), meaning the pose of the feature k in the camera-body frame at the time t. However, the solved s_(k,t) is not the real depth of the point p_(k,t)^(C_t ) in the world frame because of the scale uncertainty in monocular vision. This is because multiplying a nonzero constant in (7.67) will not change the result of s_(k,t). 
A very naive SLAM can therefore be achieved using the equations below.
	p_k^L=T_(C_0)^L ∏_t^0▒T_(C_t)^(C_(t-1) )  p_(k,t)^(C_t )	(7.68)
	M^L={p_1^L⋯p_(N_feature)^L }	(7.69)
However, its performance is not reliable and the results are prone to drift quickly. The fact that a feature can be detected in multiple epochs is not considered in this naive SLAM. Bundle adjustment is usually applied to tackle this quickly drifting problem. 
Bundle Adjustment for multi-epoch SLAM
Finally, a batch optimization is applied to achieve a bundle adjustment of the map. Here, instead of using time epoch to index the image, an image index i is used.
	〖{R〗_i,t_i,p_k^(C_i )}=argmin┬(R_i,t_i,p_k^(C_i ) )⁡∑_(i∈〖Img〗_l)▒∑_(k∈Feature)▒‖p_(i,k)^(pixel_i )-π(R_i p_k^(C_i )+t_i)‖^2 	(7.70)
where 〖Img〗_l are the set of co-visible images, 〖Feature〗_k are all the feature points can be seen in these images I_l. π(∙) is the projection function from camera-body frame to pixel frame, meaning (7.40), pCi is the 3D point of k-th feature in camera-body frame when i-th image is taken, p_k^(C_i ) is the 3D point of k-th feature in camera-body frame when i-th image is taken, p_(i,k)^(pixel_i ) is the detected feature point in image I_i corresponding with the point p_k^(C_i ). By re-projecting feature k to image i,  considering the rotation matrix R_i and translation vector t_i, the predicted pixel position can be found. Then, the transformation matrix that can minimize the Euclidean distance between the predicted and measured pixel position of the feature is the optimized relative positioning. The map with the consideration of bundle adjustment can be finally obtained using (7.68). Note the R_i, t_i estimated using the bundle adjustment are much more reliable than that calculated using (7.51-7.57). In fact, the rough estimation by (7.51-7.57) is usually only used as an initial guess for (7.70).

7.4.3	RGB-D camera
A popular RGB-D camera (such as Microsoft Kinect) is to integrate a RGB camera with an infrared (IR) sensor as shown in the following figure. The IR sensor is a radiation-sensitive optoelectronic component, which can measure the distance of an object. It uses an IR LED to send an IR light and a photodiode to sense the reflected light. Similar to a LiDAR, the two-way TOF can therefore be used to measure the distance (or usually called “depth” in image processing field) between the camera and an object. Due to the integration of RGB camera and IR sensor, each pixel also includes its depth information. The principle of integration, in fact, is similar to Section 7.2.4. Since the depths are provided by a RGB-“D” camera, its visual odometry and SLAM are very similar to that of the LiDAR ones mentioned in the previous section. The RGB part is similar to that of monocular camera. The idea of ICP, NDT and LOAM can be directly applied. However, it is usually more challenging compared to that using LiDAR. It is because:
	Higher noise in the depth measurement generated by the IR sensor comparing to that of a LiDAR. One of the popular solutions to improve the performance of the SLAM using RGB-D camera is to combine the pixel feature with the depth information in the SLAM. It is a PCD SLAM that integrates with a monocular camera based SLAM. Effective integration depends on accurate knowledge of the information matrices of both systems. An adaptive approach is preferred but it is also one of the challenges for this integration.
	Narrower field of view (FoV) of a camera, compared to a mechanical LiDAR. In fact, it is similar to a solid-state LiDAR. The narrower FoV usually causes the decrease of the observability of the model, meaning a larger positioning error or a higher chance of obtaining a local minimum solution. A common approach to deal with the lower observability problem is to provide a better initial guess to the solution for the nonlinear optimization problem. However, this is a chicken and egg problem. For SLAM, an integration with IMU is an efficient approach to serve for this purpose since IMU can measure the propagation of the agent’s pose.

7.4.4	Challenges of a Visual SLAM
For monocular SLAM, the biggest bottleneck is the inability to determine the true scale of the scene, which makes the SLAM system relying only on a single camera severely limited in practical applications. Stereo SLAM can compute the depth information, but its accuracy is greatly dependent on the resolution of cameras and the baseline length between cameras, and the calculation of depth of each pixel is computationally time-consuming and unreliable. The integration of camera and IMU can effectively solve the problem of scale [10]. The RGB-D camera can avoid the large computation required for depth acquisition, but the range is usually small, and they are noisy and weak against interference. Therefore, it is commonly used indoors rather than in outdoor environments.
For the monocular-camera based SLAM, feature tracking plays an important role in determining the performance of data association for the motion estimation. The objective of feature tracking is to find the correct feature matching between consecutive series of images. The following four are the challenges for rich and stable feature detection and checking.
	Illumination change: Both optical flow-based feature tracking method (Lucas-Kanade method) and descriptor-based (ORB descriptor) rely on the image brightness constancy. Specifically, the optical flow-based relies heavily on the assumption of brightness constancy to find the feature correspondences between two consecutive images. However, the assumption of the image brightness constancy in practical scene may not always be satisfied due to lighting conditions and environment structures.
	Imaging blur: The image blur caused by large motion or sharp turn (large accelerations) will degrade the quality of features. In other words, the image blur leads to incorrect feature detection and tracking.
	Dynamic objects: The dynamic objects violate the assumption that the surrounding features are static  
	for the SLAM. Specifically, dynamic objects violate the assumption of optical-flow tracking algorithm that the pixels around the key points have same motion.
	Low-textured environment: If the environment captured is a white wall without any corner points to be detected, then no features can be detected and tracked.
	Environment with repetitive features: If the multiple images taken at different location are similar, then the optic flow will mistakenly give a near zero velocity. For example, the corridor of hotel rooms is a typical scenario that visual SLAM will drift very quickly.

7.4.5	Integration of a Camera with LiDAR SLAM
If the expected LiDAR SLAM accuracy is higher than that of a visual SLAM, it is popular to map the pixels of an image into the accumulated LiDAR PCD map. This requires very precise calibrations on both the intrinsic parameters of the camera and the extrinsic parameters between the LiDAR and camera body coordinate systems. The time synchronization between camera and LiDAR is also required to be carefully handled. Assuming the calibrations are done precisely, the pixel (which the RGB value) of an image can be transformed into a global coordinate system using the LiDAR SLAM. The idea is to give a RGB color to the PCD map generated by LiDAR SLAM as described in the Figure 7.24.
 
Figure 7.24: The flowchart of transforming pixels to a LiDAR frame.
Once the pixels are transformed into the LiDAR frame, the RGB value of the pixel closest to a PCD point is used to color that PCD point. Note that the field of view (FoV) of the camera and LiDAR may differ. To ensure accurate integration, only PCD points and pixels within the overlapping FoV are used for data fusion.
 


Roles of IMU in LiDAR SLAMs
Most of the SLAM methods are challenged by a practical aspect, the computational loads. The size of the model meaning the size of a matrix used in the linear algebra calculation. As the size of the matrix increases, the computational loads of calculating the inverse, the Jacobian matrix will increase drastically [37]. Thus, a small size model which can maintain its observability is preferred. In a real system, a threshold on the numbers of the features used is usually pre-determined to avoid the excessive computational load. The selection of the features used can be done using RANSAC [38]. However, there are cases that the observability is compromised after the selection of the features. As mentioned in Section 7.2.2, IMU is a good candidate to provide a good initial guess to mitigate the challenge of the small model observability. IMUs measure the 3D accelerations and the 3D rotational rates which can describe the propagation of the agent’s pose in a short time. Here, how short it depends on the grade of the IMU used. For example, a MEMS-level IMU used in a smartphone can survive for a few seconds. In addition, if the SLAMs are calculated whenever the LiDAR PCD and images are received, the computational load could be too large for some IPIN applications. The rough output frequencies of LiDAR and image could be 10Hz and 15Hz, respectively, meaning SLAM calculation needs 10+ times in a second, which may not be very meaningful if the movement of the agent is not fast. At this time, IMU can also be used to predict the motion of the agent, and the prediction can be used as an initial guess before the calculation of the odometry and SLAM. This can effectively reduce the output rate of the SLAM while maintaining accuracy. It is important to note that the update rate of IMU is very high, which can reach 400Hz. If the INS mechanization is used 400Hz of the update rate, the computational load is high as well. Thus, a pre-integration method is popularly used to solve this problem. 
For the LiDAR and INS integration, the authors recommend LIO-SAM [8] and its open-source code is available at (https://github.com/TixiaoShan/LIO-SAM). For the Visual odometry and INS integration, the authors recommend VINS [10] and its open-source code is available at (https://github.com/HKUST- Aerial-Robotics/VINS-Mono).

Conclusions
 
This chapter has provided a comprehensive and systematic overview of indoor SLAM technologies, emphasizing their mathematical foundations, algorithmic advancements, and practical implementations. The chapter traces the historical development of SLAM, highlighting its evolution from classical filter-based approaches to modern optimization-based methods. Key milestones such as EKF-SLAM, FastSLAM, GraphSLAM, and recent robust perception frameworks have been discussed, illustrating how each era built upon the strengths of its predecessors while addressing their limitations.
The general mathematical model of SLAM has been detailed, outlining the core principles of feature extraction, transformation estimation, and batch optimization. This foundational framework has supported in-depth discussions on LiDAR SLAM, Visual SLAM, and the integration of IMUs. LiDAR SLAM has been explored through methods such as ICP, NDT, and LOAM, each offering distinct advantages in terms of accuracy, computational efficiency, and robustness. Visual SLAM has been examined in the context of monocular and RGB-D cameras, with a focus on feature detection, tracking, and the challenges posed by scale uncertainty and dynamic environments. The critical role of IMUs in enhancing SLAM performance has been emphasized, particularly in providing initial guesses for pose estimation and mitigating computational burdens.
The integration of LiDAR, cameras, and IMUs has been demonstrated as a powerful approach for achieving robust and accurate indoor mapping and localization. By leveraging the complementary strengths of these sensors — LiDAR's precise depth measurements, cameras' rich visual information, and IMUs' high-frequency motion estimates — modern SLAM systems can operate effectively in complex, dynamic environments. Comparative examples of LiDAR SLAM algorithms have further illustrated the practical implications of these technologies, demonstrating their potential for real-world applications.
Looking ahead, the next chapter will build upon the concepts introduced in this chapter by exploring advanced sensor fusion techniques. The integration of additional sensors and data sources to further enhance the reliability, accuracy, and adaptability of SLAM systems will be investigated. This will include discussions on adaptive fusion strategies, the handling of challenging scenarios such as dynamic objects and resource-constrained environments. The insights gained from this chapter help readers grasp the fundamental concepts and methodologies underlying indoor SLAM technologies. These insights lay the groundwork for understanding the sophisticated sensor fusion frameworks that are currently being explored in the advancement of SLAM systems. By building on this foundation, readers will be better equipped to comprehend the complex sensor fusion techniques discussed in the next chapter, which aim to enhance the reliability, accuracy, and adaptability of SLAM systems for applications in indoor navigation and other relevant fields.
 
Appendix: Derivation of Epipolar Constraint
From (7.46), the transformation can be represented by
	s_(k,t+∆t) p_(k,t+∆t)^pixel=K(Rp_(k,t)^(C_t )+t)	(7.71)
	s_(k,t+∆t) p_(k,t+∆t)^pixel=KRp_(k,t)^(C_t )+Kt	(7.72)
Then, the inverse transformation is applied from the camera-body coordinate to the pixel plane: 
	s_(k,t+∆t) p_(k,t+∆t)^pixel=KRK^(-1) s_(k,t) p_(k,t)^pixel+Kt	(7.73)
	K^(-1) s_(k,t+∆t) p_(k,t+∆t)^pixel=RK^(-1) s_(k,t) p_(k,t)^pixel+t	(7.74)
After that, we can use the property that for any vectors a and b, a×b=a^⋀ b, where a^⋀ is the skew-symmetric matrix of a.
Multiplying both sides by  t^⋀:
	t^⋀  K^(-1)  s_(k,t+∆t)  p_(k,t+∆t)^pixel=t^⋀ RK^(-1)  s_(k,t)  p_(k,t)^pixel+t^⋀ t	(7.75)
	Since t^⋀ t=0 (cross product of a vector with itself is zero), we have:
	t^⋀  K^(-1)  s_(k,t+∆t)  p_(k,t+∆t)^pixel=t^⋀ RK^(-1)  s_(k,t)  p_(k,t)^pixel	(7.76)
Taking the dot product with K^(-1) p_(k,t)^pixel on both sides:
	(K^(-1) p_(k,t)^pixel )^T t^⋀  K^(-1)  s_(k,t+∆t)  p_(k,t+∆t)^pixel=(K^(-1) p_(k,t)^pixel )^T t^⋀ RK^(-1)  s_(k,t)  p_(k,t)^pixel	(7.77)
The left side is zero because (K^(-1) p_(k,t)^pixel )^T t^⋀  K^(-1) p_(k,t+∆t)^pixel is the dot product of a vector with its own cross product, which is always zero.
Therefore:
	0=(K^(-1) p_(k,t)^pixel )^T t^⋀ RK^(-1)  s_(k,t)  p_(k,t)^pixel	(7.78)
Since s_(k,t) is a scalar, we can rewrite:
	0=s_(k,t) (K^(-1) p_(k,t+∆t)^pixel )^T t^⋀ RK^(-1) p_(k,t)^pixel	(7.79)
For this equation to be true for any nonzero s_(k,t), we must have:
	(K^(-1) p_(k,t+∆t)^pixel )^T t^⋀ RK^(-1) p_(k,t)^pixel=0	(7.80)
This is the epipolar constraint equation.
 
Appendix: List of Abbreviations
Abbreviation	Meaning
SLAM	Simultaneous Localization and Mapping
LiDAR	Light Detection and Ranging
IMU	Inertial Measurement Unit
ORB	Oriented FAST and Rotated BRIEF
PCD	point cloud data
FoV	Field of View
ICP	Iterative Closest Point
NDT	Normal Distributions Transform
TSDF	Truncated Signed Distance Function
FAST	Features from Accelerated Segment Test
PnP	Perspective-n-Point
SIFT	Scale-Invariant Feature Transform
BoW	Bag of Words
EKF-SLAM	Extended Kalman Filter based SLAM
PTAM	Parallel Tracking and Mapping
LSD-SLAM	Large-Scale Direct Monocular SLAM
ORB-SLAM	Oriented FAST and Rotated BRIEF based SLAM
VINS-Mono	Monocular Visual-Inertial Systems
LOAM	Lidar Odometry and Mapping
LIO-SAM	Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping
 
Bibliography

	R. C. Smith and P. Cheeseman, “Estimating uncertain spatial relationships in robotics,” Autonomous robot vehicles, pp. 167–193, 1990.
	M. Montemerlo, S. Thrun, D. Koller, B. Wegbreit, et al., “Fastslam: A factored solution to the simultaneous localization and mapping problem,” in Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 593–598, 2002.
	F. Dellaert, “Square root sam: Simultaneous localization and mapping via square root information smoothing,” in The International Journal of Robotics Research, vol. 25, pp. 1181–1203, 2006.
	S. Thrun and M. Montemerlo, “The graphslam algorithm with applications to large-scale mapping of urban structures,” in The International Journal of Robotics Research, vol. 25, pp. 403–429, 2006.
	R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a versatile and accurate monocular slam system,” IEEE transactions on robotics, vol. 31, no. 5, pp. 1147–1163, 2015.
	J. Engel, T. Sch¨ops, and D. Cremers, “Lsd-slam: Large-scale direct monocular slam,” in European conference on computer vision, pp. 834–849, Springer, 2014.
	J. Zhang and S. Singh, “Loam: Lidar odometry and mapping in real-time,” in Robotics: Science and Systems, vol. 2, 2014.
	T. Shan, B. Englot, D. Meyers, W. Wang, C. Ratti, and D. Rus, “Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping,” in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5135–5142, IEEE, 2020.
	G. Klein and D. Murray, “Parallel tracking and mapping for small ar workspaces,” in 2007 6th IEEE and ACM international symposium on mixed and augmented reality, pp. 225–234, IEEE, 2007.
	T. Qin, P. Li, and S. Shen, “Vins-mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, vol. 34, no. 4, pp. 1004–1020, 2018.
	R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. Kelly, and A. J. Davison, “Slam++: Simultaneous localisation and mapping at the level of objects,” Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1352–1359, 2013.
	K. Tateno, F. Tombari, I. Laina, and N. Navab, “Cnn-slam: Real-time dense monocular slam with learned depth prediction,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6243–6252, 2017.
	B. Bescos, J. M. F´abregas, J. Sol´a, and F. Moreno-Noguer, “Dynaslam: Tracking, mapping, and inpainting in dynamic scenes,” in IEEE Robotics and Automation Letters, vol. 3, pp. 4076–4083, IEEE, 2018.
	R. Mur-Artal and J. D. Tard´os, “Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras,” IEEE Transactions on Robotics, vol. 33, no. 5, pp. 1255–1262, 2017.
	H. Rebecq, T. Horstschaefer, and D. Scaramuzza, “Real-time visual-inertial odometry for event cam- eras using keyframe-based nonlinear optimization,” in British Machine Vision Conference (BMVC), 2017.
	Z. Huang, J. Rajasegaran, and A. Kanazawa, “Point-slam: Dense neural point cloud-based slam,” in Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI, pp. 597–614, Springer, 2022.
	P. J. Besl and N. D. McKay, “Method for registration of 3-d shapes,” IEEE Transactions on Pattern Analysis & Machine Intelligence, no. 2, pp. 239–256, 1992.
	R. B. Rusu and S. Cousins, “3d is here: Point cloud library (pcl),” in 2011 IEEE international conference on robotics and automation, pp. 1–4, IEEE, 2011.
	P. Biber and W. Straßer, “The normal distributions transform: A new approach to laser scan match- ing,” in Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003), vol. 3, pp. 2743–2748, IEEE, 2003.
	M. Magnusson, A. Lilienthal, and T. Duckett, “Evaluation of 3d registration reliability and speed—a comparison of icp and ndt,” 2007 IEEE International Conference on Robotics and Automation, pp. 3907–3912, 2009.
	J. Pang, K. Huang, Z. Deng, and J. Liu, “3d ndt transformation estimation with ground segmentation data,” IEEE Access, vol. 6, pp. 26838–26849, 2018.
	A. Segal, D. Haehnel, and S. Thrun, “Generalized-icp,” in Robotics: science and systems, vol. 2, p. 435, Seattle, WA, 2009.
	H. Ye, Y. Chen, and M. Liu, “Tightly coupled 3d lidar inertial odometry and mapping,” 2019 International Conference on Robotics and Automation (ICRA), pp. 3144–3150, 2019.
	P. Wen, Q. Wang, B. Sun, K. Song, and Y. Han, “Performance evaluation of a lidar inertial odom- etry and mapping (liom) method,” in 2018 IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2018), pp. 7598–7601, IEEE, 2018.
	P. Huang, M. Ding, Y. Zou, X. Zhang, and R. Ji, “Coarse-to-fine semantic localization with hd map for autonomous driving,” in 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 9664–9670, IEEE, 2021.
	H. Wang, C. Wang, and L. Xie, “F-loam: Fast lidar odometry and mapping,” 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 4390–4396, 2021.
	J. Shi and C. Tomasi, “Good features to track,” 1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 593–600, 1994.
	B. D. Lucas, T. Kanade, et al., “An iterative image registration technique with an application to stereo vision,” in IJCAI, vol. 81, pp. 674–679, 1981.
	E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An efficient alternative to sift or surf,” in 2011 International conference on computer vision, pp. 2564–2571, IEEE, 2011.
	G. Bradski and A. Kaehler, Learning OpenCV: Computer vision with the OpenCV library. ” O’Reilly Media, Inc.”, 2008.
	K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
	Z. Zhang, “A flexible new technique for camera calibration,” in IEEE Transactions on pattern analysis and machine intelligence, vol. 22, pp. 1330–1334, IEEE, 2000.
	Z. Zhang, “Determining motion from 3d line segment matches: A comparative study,” in Image and Vision Computing, vol. 16, pp. 549–565, Elsevier, 1998.
	W. Gao, X. Zhang, L. Yang, and H. Liu, Lectures on Visual Perception. Springer, 2017.
	R. I. Hartley, “In defense of the eight-point algorithm,” IEEE Transactions on pattern analysis and machine intelligence, vol. 19, no. 6, pp. 580–593, 1997.
	R. Hartley and A. Zisserman, Multiple view geometry in computer vision. Cambridge university press, 2004.
